{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1a57800054e52c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19b96e70fcd02c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:29:15.350252Z",
     "start_time": "2024-10-17T14:29:11.997184Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import flip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899eb87929addf03",
   "metadata": {},
   "source": [
    "# Device and extracted features location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c19c62b2ec1cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:29:18.892917Z",
     "start_time": "2024-10-17T14:29:18.697436Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8393d81391d0d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:29:20.204700Z",
     "start_time": "2024-10-17T14:29:20.198891Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_image(scene_name, image_number, ss, data):\n",
    "    wh = ss[scene_name]\n",
    "    plt.imshow(data[scene_name][image_number].reshape((wh[1], wh[0], 3))[::-1])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f552ab28d17540",
   "metadata": {},
   "source": [
    "# Dataset Reading and creation\n",
    "Read image names, width, height "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadeb31d44d3184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:29:38.290767Z",
     "start_time": "2024-10-17T14:29:37.680325Z"
    }
   },
   "outputs": [],
   "source": [
    "scenes = ['train_scene', 'truck_scene', 'drjohnson_scene', 'playroom_scene']\n",
    "scene_adjust = {'drjohnson_scene':True, 'playroom_scene':True, 'train_scene':False, 'truck_scene':False}\n",
    "scene_paths = {'drjohnson_scene': {}, 'playroom_scene': {}, 'train_scene': {}, 'truck_scene': {}}\n",
    "scenes_size = {'drjohnson_scene': [], 'playroom_scene':[], 'train_scene': [], 'truck_scene': []}\n",
    "scenes_files = {'drjohnson_scene': [], 'playroom_scene':[], 'train_scene': [], 'truck_scene': []}\n",
    "scenes_sample = {'drjohnson_scene': [], 'playroom_scene':[], 'train_scene': [], 'truck_scene': []}\n",
    "scenes_gt = {'drjohnson_scene': [], 'playroom_scene':[], 'train_scene': [], 'truck_scene': []}\n",
    "\n",
    "for scene in scenes:\n",
    "    paths = {'dataset_path':'', 'feature_path':'', 'images_path':'', 'gsres_path':''}\n",
    "    dataset_path = f'...'\n",
    "    feature_path = f'...'\n",
    "    images_path = f'...'\n",
    "    gsres_path = f'...'\n",
    "\n",
    "    paths['dataset_path'] = dataset_path\n",
    "    paths['feature_path'] = feature_path\n",
    "    paths['images_path'] = images_path\n",
    "    paths['gsres_path'] = gsres_path\n",
    "    scene_paths[scene] = paths\n",
    "    \n",
    "    image_names = open(f'{feature_path}imagenames', 'r')\n",
    "    image_names_file = image_names.read().splitlines()\n",
    "    width_height = image_names_file[0].split(' ')\n",
    "    image_width, image_height = int(width_height[0]), int(width_height[1])\n",
    "    files_list = image_names_file[1:]\n",
    "    scenes_size[scene] = [image_width, image_height]\n",
    "    scenes_files[scene] = files_list\n",
    "for scene in scenes:\n",
    "    print(f'{scene}:')\n",
    "    print(f'\\tPaths:\\n\\t\\t{scene_paths[scene]}')\n",
    "    print(f'\\tAdjust: {scene_adjust[scene]}')\n",
    "    print(f'\\tSize: {scenes_size[scene]}')\n",
    "    print(f'\\t# of images: {len(scenes_files[scene])}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c7d5614e2e4f1",
   "metadata": {},
   "source": [
    "# Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be004349bdbf481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:29:41.860510Z",
     "start_time": "2024-10-17T14:29:41.852196Z"
    }
   },
   "outputs": [],
   "source": [
    "image_sample = 0.5\n",
    "for scene in scenes:\n",
    "    sampled_images = random.sample(range(len(scenes_files[scene])), int(len(scenes_files[scene]) * image_sample))\n",
    "    scenes_sample[scene] = sampled_images\n",
    "    \n",
    "for scene in scenes:\n",
    "    print(f'{scene}:')\n",
    "    print('\\t# of sampled images:', len(scenes_sample[scene]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efd789abf264d7",
   "metadata": {},
   "source": [
    "## Read ground truths and features\n",
    "\n",
    "Some ground truth images are double the size so we must adjust their scale accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae6f79ae970032c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:30:09.987688Z",
     "start_time": "2024-10-17T14:29:46.553173Z"
    }
   },
   "outputs": [],
   "source": [
    "gt = {'drjohnson_scene': [], 'playroom_scene':[], 'train_scene': [], 'truck_scene': []}\n",
    "for scene in scenes:\n",
    "    ground_truths = [np.array(ImageOps.scale(ImageOps.mirror(Image.open(f'{scene_paths[scene]['images_path']}{scenes_files[scene][i]}.jpg').rotate(180)), 1 if not scene_adjust[scene] else 0.5)).reshape((-1, 3)).astype(np.float32) for i in scenes_sample[scene]]\n",
    "    ground_truths = [i/255.0 for i in ground_truths]\n",
    "    gt[scene] = ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4bd0fc6f95b31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:30:10.200406Z",
     "start_time": "2024-10-17T14:30:10.004384Z"
    }
   },
   "outputs": [],
   "source": [
    "show_image('playroom_scene', 0, scenes_size, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9600495de41b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:31:21.941802Z",
     "start_time": "2024-10-17T14:30:10.246191Z"
    }
   },
   "outputs": [],
   "source": [
    "bf = {'drjohnson_scene': [], 'playroom_scene':[], 'train_scene': [], 'truck_scene': []}\n",
    "for scene in scenes:\n",
    "    blending_features = [pd.read_csv(f'{scene_paths[scene]['feature_path']}blend_{scenes_files[scene][i]}', sep=' ', header=None) for i in scenes_sample[scene]]\n",
    "    bf[scene] = blending_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314c210924ee8842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:35:51.252733Z",
     "start_time": "2024-10-17T14:31:21.981208Z"
    }
   },
   "outputs": [],
   "source": [
    "fr = {'drjohnson_scene': [], 'playroom_scene':[], 'train_scene': [], 'truck_scene': []}\n",
    "for scene in scenes:\n",
    "    features_rest = [pd.read_csv(f'{scene_paths[scene]['feature_path']}features_{scenes_files[scene][i]}', sep=' ', header=None) for i in scenes_sample[scene]]\n",
    "    fr[scene] = features_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd68aa-e9cc-4afc-800f-237aeff049fb",
   "metadata": {},
   "source": [
    "## Sample Datasets\n",
    "\n",
    "Discard image_sample percent of images and for each image discard pixel_sample percent of pixels to create the train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7548043-8232-4465-ad11-a172735dc5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T17:47:39.295623Z",
     "start_time": "2024-10-17T17:46:25.815545Z"
    }
   },
   "outputs": [],
   "source": [
    "pixel_sample = 0.1\n",
    "X_train = {'drjohnson_scene': pd.DataFrame(), 'playroom_scene':pd.DataFrame(), 'train_scene': pd.DataFrame(), 'truck_scene': pd.DataFrame()}\n",
    "y_train = {'drjohnson_scene': pd.DataFrame(), 'playroom_scene':pd.DataFrame(), 'train_scene': pd.DataFrame(), 'truck_scene': pd.DataFrame()}\n",
    "for scene in scenes:\n",
    "    X = pd.DataFrame()\n",
    "    y = pd.DataFrame()\n",
    "    for i in range(len(scenes_sample[scene])):\n",
    "        sum_aici = fr[scene][i].iloc[:, :3].to_numpy()\n",
    "        sum_aici_k = fr[scene][i].iloc[:, 3:6].to_numpy()\n",
    "        sum_ai = fr[scene][i].iloc[:, 6:7].to_numpy()\n",
    "        sum_ai_k = fr[scene][i].iloc[:, 7:8].to_numpy()\n",
    "        prod_1_ai = fr[scene][i].iloc[:, 8:9].to_numpy()\n",
    "        prod_1_ai_k = fr[scene][i].iloc[:, 9:10].to_numpy()\n",
    "        o_m_z = fr[scene][i].iloc[:, 10:11].to_numpy()\n",
    "        zmin = fr[scene][i].iloc[:, 11:12].to_numpy()\n",
    "        zmax = fr[scene][i].iloc[:, 12:13].to_numpy()\n",
    "\n",
    "        ctail = (sum_aici - sum_aici_k)/(sum_ai - sum_ai_k + 0.00000000001) #3\n",
    "        prod_ai_k_n = prod_1_ai/(prod_1_ai_k + 0.00000000001)\n",
    "        tail = prod_1_ai_k * ((np.ones_like(prod_ai_k_n) - prod_ai_k_n)*ctail)\n",
    "\n",
    "        blendk = bf[scene][i] #3\n",
    "        prod_1_ai_k_df = pd.DataFrame(prod_1_ai_k) #1\n",
    "        z_norm = pd.DataFrame((o_m_z - zmin)/(zmax - zmin + 0.00000000001)) #1\n",
    "        ctail = pd.DataFrame(ctail) #3\n",
    "        prod_ai_k_n = pd.DataFrame(prod_ai_k_n) #1\n",
    "        tail = pd.DataFrame(tail)\n",
    "    \n",
    "        sampled_pixels = random.sample(range(len(blendk)), int(len(blendk) * pixel_sample))\n",
    "        feats = pd.concat([ctail, tail, z_norm], axis=1).iloc[sampled_pixels, :]\n",
    "        ground = pd.DataFrame(gt[scene][i]).iloc[sampled_pixels, :]\n",
    "    \n",
    "        X = pd.concat([X, feats])\n",
    "        y = pd.concat([y, ground])\n",
    "    X_train[scene] = X\n",
    "    y_train[scene] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b133867ace19024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T17:47:43.476756Z",
     "start_time": "2024-10-17T17:47:43.462480Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['playroom_scene']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429872f5dc586d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Model Architecture\n",
    "Fully connected MLP with 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb02d1913d6b63e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:45:23.663366Z",
     "start_time": "2024-10-17T15:45:23.651574Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class GSNN(nn.Module):\n",
    "    def __init__(self, in_features, s1, s2, s3, out_features, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.arch = nn.Sequential(nn.Linear(in_features, s1, device=self.device), nn.ReLU(),\n",
    "                                  nn.Linear(s1, s2, device=self.device), nn.ReLU(),\n",
    "                                  nn.Linear(s2, s3, device=self.device), nn.ReLU(),\n",
    "                                  nn.Linear(s3, out_features, device=self.device), nn.Sigmoid())\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s1 = s1\n",
    "        self.s2 = s2\n",
    "        self.s3 = s3\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.arch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fe83e0cd6f18a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training Loop\n",
    "Adam optimizer, smooth L1 loss and backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad906ce007845fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T17:47:49.022117Z",
     "start_time": "2024-10-17T17:47:49.013810Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "def train(model, X_train, y_train, epochs=700, batch_size=100, lr=0.01):\n",
    "    #Create dataset\n",
    "    X_tensor = torch.tensor(X_train.to_numpy(dtype=np.float32)).to(device)\n",
    "    y_tensor = torch.tensor(y_train.to_numpy(dtype=np.float32)).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        progress = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}', unit='batches')\n",
    "\n",
    "        for b_train, b_labels in progress:\n",
    "            outputs = model(b_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, b_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress.set_postfix({'Loss': total_loss / len(dataloader)})\n",
    "        train_loss.append(total_loss/len(dataloader))\n",
    "\n",
    "    print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370287cf293e359",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Model Creation and Training\n",
    "MLP architecture 10-16-32-16-3. Train for 25 epochs with a batch of 512 and learning rate 0.001. Save trained model weigths in a .pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7e53a-4763-48b9-8083-5b1ed26e7317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:14:42.867824Z",
     "start_time": "2024-10-17T17:47:55.030567Z"
    }
   },
   "outputs": [],
   "source": [
    "model = GSNN(7, 16, 32, 16, 3, device)\n",
    "epochs = 25\n",
    "batch = 512\n",
    "lr = 0.001\n",
    "train(model, X_train['playroom_scene'], y_train['playroom_scene'], epochs=epochs, batch_size=batch, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7905d44786367cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T20:58:10.676692Z",
     "start_time": "2024-10-15T20:58:10.670391Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'full_{model.in_features}_{model.s1}_{model.s2}_{model.s3}_{model.out_features}_{epochs}_{batch}_{lr}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99fe4c10632708",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "Load model from previously saved file and enable evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9832d7aaa2598bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:02:03.476711504Z",
     "start_time": "2024-10-15T20:58:12.100482Z"
    }
   },
   "outputs": [],
   "source": [
    "gsnn = GSNN(10, 16, 32, 16, 3, device)\n",
    "gsnn.to(device)\n",
    "model_path = f'full_{model.in_features}_{model.s1}_{model.s2}_{model.s3}_{model.out_features}_{epochs}_{batch}_{lr}.pt'\n",
    "gsnn.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "gsnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca4231983eb634",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Choose 10 sample ground truth images and their respective features, feed into the model and save outputs as vesus images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20737ea86192d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:02:03.477411083Z",
     "start_time": "2024-10-15T20:58:12.389453Z"
    }
   },
   "outputs": [],
   "source": [
    "scene = 'truck_scene'\n",
    "sample_images = 10\n",
    "sample_out = random.sample(range(len(scenes_files[scene])), sample_images)\n",
    "for n in sample_out:\n",
    "    image_name = scenes_files[scene][n]\n",
    "    \n",
    "    blend = pd.read_csv(f'{scene_paths[scene]['feature_path']}blend_{image_name}', sep=' ', header=None)\n",
    "    feats = pd.read_csv(f'{scene_paths[scene]['feature_path']}features_{image_name}', sep=' ', header=None)\n",
    "\n",
    "    sum_aici = feats.iloc[:, :3].to_numpy()\n",
    "    sum_aici_k = feats.iloc[:, 3:6].to_numpy()\n",
    "    sum_ai = feats.iloc[:, 6:7].to_numpy()\n",
    "    sum_ai_k = feats.iloc[:, 7:8].to_numpy()\n",
    "    prod_1_ai = feats.iloc[:, 8:9].to_numpy()\n",
    "    prod_1_ai_k = feats.iloc[:, 9:10].to_numpy()\n",
    "    o_m_z = feats.iloc[:, 10:11].to_numpy()\n",
    "    zmin = feats.iloc[:, 11:12].to_numpy()\n",
    "    zmax = feats.iloc[:, 12:13].to_numpy()\n",
    "\n",
    "    ctail = (sum_aici - sum_aici_k)/(sum_ai - sum_ai_k + 0.00000000001) #3\n",
    "    prod_ai_k_n = prod_1_ai/(prod_1_ai_k + 0.00000000001)\n",
    "    tail = prod_1_ai_k * ((np.ones_like(prod_ai_k_n) - prod_ai_k_n)*ctail)\n",
    "    \n",
    "    prod_1_ai_k_df = pd.DataFrame(prod_1_ai_k) #1\n",
    "    z_norm = pd.DataFrame((o_m_z - zmin)/(zmax - zmin + 0.00000000001)) #1\n",
    "    ctail = pd.DataFrame(ctail) #3\n",
    "    prod_ai_k_n = pd.DataFrame(prod_ai_k_n) #1\n",
    "    tail = pd.DataFrame(tail)   \n",
    "    \n",
    "    inp = pd.concat([blend, ctail, tail, z_norm], axis=1)\n",
    "    \n",
    "    input_tensor = torch.tensor(inp.values, dtype=torch.float32).to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = gsnn(input_tensor)\n",
    "    df = pd.DataFrame(predictions.cpu().numpy())\n",
    "\n",
    "    output_image = df.to_numpy().reshape((scenes_size[scene][1], scenes_size[scene][0], 3))\n",
    "    gt = ImageOps.scale(Image.open(f'{scene_paths[scene]['images_path']}{image_name}.jpg'), 1 if not scene_adjust[scene] else 0.5)\n",
    "    gsres = ImageOps.scale(Image.open(f'{scene_paths[scene]['gsres_path']}{image_name}.png'), 1 if not scene_adjust[scene] else 0.5)\n",
    "\n",
    "    plt.imshow(np.array(gsres)/255.0)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{image_name}_3dgs.png', dpi=250, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    plt.imshow(np.array(gt)/255.0)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{image_name}_actual.png', dpi=250, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    plt.imshow(output_image[::-1])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{image_name}_inference.png', dpi=250, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0862ed43dabf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:02:03.477691049Z",
     "start_time": "2024-10-16T10:47:21.200538Z"
    }
   },
   "outputs": [],
   "source": [
    "for n in sample_out:\n",
    "    image_name = scenes_files[scene][n]\n",
    "    fl, _, _ = flip.evaluate(f'{image_name}_actual.png', f'{image_name}_inference.png', \"LDR\")\n",
    "    plt.imshow(fl)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{image_name}_gt_flip.png', dpi=250, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    fl, _, _ = flip.evaluate(f'{image_name}_actual.png', f'{image_name}_inference.png', \"LDR\")\n",
    "    plt.imshow(fl)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{image_name}_3dgs_flip.png', dpi=250, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    fl, _, _ = flip.evaluate(f'{image_name}_actual.png', f'{image_name}_3dgs.png', \"LDR\")\n",
    "    plt.imshow(fl)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{image_name}_gt_3dgs_flip.png', dpi=250, bbox_inches='tight', pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
